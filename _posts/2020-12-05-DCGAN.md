---
layout: post
title: DCGAN 논문 리뷰 - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (ICLR 2016)
subtitle: DCGAN paper review - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (ICLR 2016)
category: [GAN]
tags: [GAN]
comments: true
use_math: true

---





> 안녕하세요. 모카의 머신러닝 입니다. 이번 포스팅은 많은 GAN 모델의 기초가 되는 DCGAN 논문인 "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, ICLR 2016"에 대해 리뷰합니다. 영어로 된 논문을 한글로 같이 해석하며 논문에서 의미하는 것이 무엇인지 같이 공부하며 이해하고 활용하실 수 있도록 준비해보았습니다. 

<br>



<em><strong>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, Alec Radford et al, ICLR 2016</strong></em>  [Paper link](https://arxiv.org/pdf/1406.2661.pdf)

논문에 대한 해석은 간결한 문장으로 표현하려고 합니다. 그림과 표는 모두 해당 논문을 참조하였습니다.

<br>





# Abstract

최근 몇년동안, 컨볼루션 네트워크의 지도학습은 컴퓨터 비전 응용에 크게 적용되어왔다. 비교할만하게, CNN의 비지도학습은 덜 주목을 받고있다. 본 논문에서는, 우리는 지도 학습을 위한 CNN의 성공과 비지도 학습의 차이를 줄이는 것을 바란다. 우리는 특정한 아키텍쳐 조건이 있고, 그리고 이것들이 비지도 학습의 막강한 후보가 될 수 있다는 것을 증명하하는 deep  convolutional generative adversarial networks (DCGANs) 이라는 CNN의 일종을 소개한다. 다양한 이미지 데이터셋에서 학습을 하며, 우리는 deep convolutional adversarial pair가 생성자와 판별자 모두에서 물체 파트에서 장면으로 계층적인 표현을 학습한다는 것을 보여준다. 추가적으로, 우리는 일반적인 이미지 표현 문제와 같은 응용을 증명하기 위하여 참신한 task들을 위해 학습된 피쳐들을 사용했다. 

# 1. Introduction

레이블이 없는 데이터셋으로부터 재사용가능한 피쳐 표현을 학습하는 것은 활발한 연구 분야이다. 컴퓨터 비전의 맥락에서, 레이블이 없는 이미지들과 비디오들을 무제한으로 사용하면 좋은 중간 표현을 학습할 수 있고, 이미지 분류와 같은 다양한 지도 학습 문제에 사용될 수 있다. 우리는 적대적 생성 신경망 (Generative Adversarial Networks, GAN) 을 학습하여 좋은 이미지 표현들을 구성하는 방법을 제안하고, 생성자와 판별자를 지도 학습 문제들의 특징 추출기로 재사용한다. GAN은 최대 우도 방법들을 대신하는 매력을 제공한다. 하나 추가적으로 주장할 수 있는 것은 이들의 학습 과정과 휴리스틱 비용 함수(픽셀 단위의 독립적인 평균 제곱 오차와 같은)의 부족은 표현 학습에 더 매력적이다. GAN은 학습하기에 불안정하고, 종종 생성자가 무의미한 이미지를 생성하는 결과를 얻기도 한다. GAN을 이해하고 GAN이 학습하는 것이 무엇인지와 다층 레이어 GAN의 중간 표현을 시각화하는 시도의 연구는 매우 제한적인 부분만 출반되었다.

본 논문에서는 다음과 같은 기여를 한다.

- 우리는 컨볼루션 GAN 아키텍쳐 형태에 대한 제약 조건들을 제안하고 평가하여 대부분의 셋팅들에서 학습이 안정화 되도록 한다. 우리는 이러한 구조의 분류를 Deep Convolutional GANs (DCGAN) 이라 말한다.
- 우리는 이미지 분류 문제를 위한 학습된 판별자를 사용하고, 다른 비지도 학습 알고리즘들에 경쟁적인 성능을 보여준다.
- 우리는 GAN에 의해 학습된 필터를 시각화하고 특정한 필터들이 특정한 객체들을 학습하는 것을 실험적으로 보여준다.
- 우리는 생성자가 생성된 샘플들의 많은 시맨틱 퀄리티를 쉽게 조절하도록 하는 흥미로운 벡터 산술 특성을 가지고 있다는 것을 보여준다.

 

# 2. Related work

## 2.1 Representation Learning From Unlabeled Data

비지도 학습은 일반적은 컴퓨터 비전 연구와 이미지의 맥락에서 상당히 잘 연구된 문제이다. 비지도 표현 학습의 전통적인 접근법은 데이터에 클러스터링을 하는것, 그리고 분류 스코어 향상에 클러스터의 이점을 사용하는 것이다. 이미지의 맥락에서는, 파워풀한 이미지 표현을 학습하기 위해서 이미지 패치의 계층적 클러스터링 하는 것이 가능하다. 또 다른 인기있는 방법은 오토 인코더를 학습하는 것이고, 이는 부호에서 무엇이 그리고 어디의 구성요소를 분리하고, 이미지를 컴팩트한 부호로 만들도록 인코딩 하는 ladder structures를 학습하거나, 가능한 정확하게 부호를 재구성된 이미지로 디코딩 하는 것이 있다. 이러한 방법들은 이미지 픽셀들로 부터 좋은 특징 표현을 학습하는 것을 보여주고 있다. Deep belief network는 계층적 표현을 학습하는 것에 잘 작동하는 것을 보여준다.

## 2.2 Generating Natural Images

이미지 생성 모델은 parametric과 non-parametric의 두 가지 카테고리고 잘 연구되어 있다.

Non-parametric 모델은 종종 존재하는 이미지들의 데이터베이스로부터 되고, 종종 이미지들의 patch를 매칭하는 것은 texture synthesis, super-resolution, in-painting과 같은 분야에 사용되어져 왔다.

이미지 생성을 위한 parametric 모델은 광범위하게 숫자 또는 texture synthesis를 위해 연구되어져 왔다. 하지만, 실제 세상에서 자연적인 이미지를 생성하는 것은 최근까지도 그렇게 성공하지는 못했다. 이미지들을 생성하기 위한 variational smapling 접근법은 약간의 성공을 이루었지만, 샘플들은 blur가 되는 현상을 겪는다. 이미지 생성을 위한 또 다른 접근법은 iterative forward diffusion process 이 있다. Generative Adversarial Networks는 잡음이 껴있고 이해할 수 없는 이미지로 고생한다. 이러한 접근법에 laplacian pyramid 확장은 더 좋은 퀄리티의 이미지들을 보여줬지만, 여러개의 모델들을 chaining 하면서 노이즈가 생겼기 때문에 아직도 물체들이 정신없어 보이는 것을 겪는다. 순환 네트워크 접근법과 deconvolution 네트워크 접근법은 또한 자연적인 이미지들을 생성하는데에 최근에 약간의 성공을 이루었다. 하지만, 이것들을 지도학습 문제를 위해 생성자를 활용하지 않았다.

## 2.3 Visualizing the internals of CNNs

뉴럴 네트워크를 사용하는 것에 대한 하나의 지속적인 비판은 이것들이 간단하게 사람이 사용할수 있는 알고리즘 형태에서 네트워크가 어떻게 동작하는지 이해를 별로 못하는 블랙박스 방법이기 때문이다. CNN은 deconvolution과 maximal activation을 필터링 하는 것으로 보여주었고, 네트워크에서 각각의 컨볼루션 필터의 목적을 대략 찾을 수 있었다. 비슷하게, 입력에 gradient descent를 사용하는 것은 이상적인 이미지는 특정한 필터들의 부분집합을 활성화하는 것을 살펴볼 수 있게 한다.

모델에 CNN을 사용하여 GAN을 스케일 업하는 역사적인 시도들은 성공적이지 않았다. 더 안정적으로 모델링 할 수 있는 반복적으로 저해상도로 생성된 이미지들을 업스케일 할 수 있도록 하는 다른 접근법을 개발하기 위해 LAPGAN의 저자로 부터 동기를 얻었다. 우리는 또한 지도학습 분야에 널리 사용된 CNN 아키텍쳐들을 사용하여 GAN을 스케일 하는 시도들에 어려움을 직면했다. 하지만, 광범위한 모델 탐험을 통해 우리는 넓은 범위의 데이터셋에 대해 안정적인 결과를 었얻고 더 고해상도의 이미지와 깊은 생성 모델을 학습할 수 있도록 하는 아키텍쳐들을 찾았다.

우리의 접근법의 핵심은 CNN 아키텍쳐를 적용하고 3개의 최근에 증명된 변경된 부분들을 변형한다.

첫번째는 결정적인 spatial pooling 함수를 strided 컨볼루션으로 대체하는 모든 컨볼루션 네트워크는 네트워크가 고유의 spatial downsampling을 학습할 수 있도록 한다. 우리는 이 접근법을 생성자에 사용하여 spatial upsampling을 학습할 수 있도록 했고, 그리고 판별자에도 이 접근법을 사용했다.

두번째는 컨볼루션 피쳐의 가장 위에 있는 full connected 레이어를 제거하는 트렌드이다. 이에 대한 가장 강력한 예시는 SOTA 이미지 분류 모델들에 사용된 global average pooling 이다. 우리는 global average pooling이 모델의 안정성을 높이지만 수렴 속도에 안좋은 영향을 미침을 알아냈다. 가장 높은 컨볼루션 피쳐들을 각각 생성자와 판별자에 입력과 출력으로 연결하는 것은 효과가 좋았다. Uniform 노이즈 분포 Z를 입력으로 받은 GAN의 첫번째 레이어는 행렬 곱셈을 통해 full connected layer라고 불릴 수 있지만, 결과적으로 4차원의 텐서로 변형되고 컨볼루션 스택의 시작으로 사용된다. 판별자에 대해서 마지막 컨볼루션 레이어는 flatten 되고 하나의 sigmoid output으로 들어간다. 모델 아키텍쳐의 예시를 보려면 그림 1의 시각화를 보자.

세번째는 입력을 zero mean과 unit variance를 가지도록 하는 각각의 unit으로 정규화하므로써 학습을 안정화하는 Batch Normalization이다. 이것은 안좋은 초기화로 부터 발생한 학습 문제를 다루는데 돕고 더 깊은 모델들에서 그래디언트의 흐름을 돕는다. 이것은 학습을 시작할 때 생성자가 GAN에서 널리 발생되는 실패하는 현상인 모든 샘플들은 하나로 붕괴하는 것으로 부터 예방하기 위한 깊은 생성자를 얻는데에 중요하다는 것이 증명되었다. 직접적으로 모든 레이어들에 batchnorm을 적용하는 것은, 한편으로, 샘플 oscillation과 모델의 불안정성의 결과를 얻었다. 이것은 생성자의 출력 레이어와 판별자의 압력 레이어에 batchnorm을 적용하는 것을 피하도록 했다.

안정적인 Deep Convolution GANs을 위한 아키텍쳐 가이드라인

- 풀링 레이어들을 strided 컨볼루션으로 대체하고 (판별자) fractional-strided 컨볼루션으로 대체한다. (생성자)
- 생성자와 판별자 둘 다에 batchnorm을 사용한다.
- 더 깊은 아키텍쳐를 위해 fully connected 히든 레이어들을 제거한다.
- 생성자의 모든 레이어에 ReLU activation을 사용하고 출력 레이어에만 Tanh를 사용한다.
- 판별자의 모든 레이어에 LeakyReLU activation을 사용한다.

![1](https://user-images.githubusercontent.com/37301677/101244541-84cbfa00-374a-11eb-9ce9-079121a0a4aa.PNG)

우리는 DCGAN을 세개의 데이터셋 LSUN, 이미지넷, 새롭게 조합된 얼굴 데이터셋에 대해 학습했다. 이들 데이터셋 각각에 대한 사용의 자세한 방법은 아래에 있다.

학습 이미지에넌 전처리가 적용되지 않았고 게다가 tanh activation 함수의 [-1, 1]의 범위로 스케일링 되었다. 모든 모델들은 미니 배치 스토캐스틱 그래디언트 경사 하강법 (SGD)로 미니 배치 사이즈 128로 학습되었다. 모든 가중치들은 zero-centered 정규 분포로 표준 편차 0.02로 초기화 되었다. LeakyReLU에서는, 모든 모델에 leak의 slope는 0.2로 설정되었다. 이전의 GAN 연구들은 학습을 가속하기 위해서 모멘텀을 사용했던 반면, 우리는 조정된 하이퍼파라미터를 사용해서 Adam 옵티마이저를 사용했다. 우리는 learning rate 0.001은 너무 크다는 것을 발견했고, 대신에 0.0002를 제안한다. 추가적으로, 우리는 모멘텀 beta1을 0.9로 두었을 때 training oscillation과 불안정성이 발견됬고, 대신에 0.5로 두었을 대 학습이 안정화 되는데 도움이 됬다.

## 4.1 LSUN

이미지 생성 모델로 부터의 샘플을의 시각적 퀄리티가 증가함으로써, 트레이닝 샘플에 대한 오버피팅과 메모라이제이션에 대한 걱정이 증가했다. 우리의 어떻게 모델이 더 많은 데이터를 스케일하고 더 고해상도의 생성하는지 증명하기 위해, 우리는 3백만장 이상의 학습 샘플들이 포함되어 있는 LSUN bedroom 데이터셋에 모델을 학습했다. 최근의 분석은 어떻게 모델이 빠르게 학습하는지와 일반화 성능 사이의 직접적인 연관이 있음을 보였다. 우리는 한번의 에폭을 학습해서 나온 샘플들이 실시간 학습을 모방한다는 것을 보였고, 게다가 수렴 이후의 샘플들은 간단한 오버피팅/메모라이징 학습 예시들을 통해서 우리의 모델이 좋은 퀄리티의 샘플들을 생성하지 못 한다는 것을 증명하는 기회도 보여준다.

### 4.1.1. Deduplication

생성자가 입력 예시들을 메모라이징 하는 것에 대한 likelihood를 더 감소시키기 위해서 우리는 간단한 이미지 중복 제거 과정을 수행한다. 우리는 3072-128-3072 denoising dropout regularized RELU 오토인코더를 트레이닝 샘플들에서 32x32로 다운샘플되고 centor-crop된 이미지로 맞춘다. 결과적인 코드 레이어 활성화는 효과적인 정보 보호 기법으로 알려진 ReLU 활성함수 임계값에 의해 이진화 되고 linear time 중복제거를 위해 허락하면서 semantic hashing의 편리한 형테를 제공한다. Hash 충돌에 대한 시각적 조사는 100개 중 1개 미만의 false positive를 측정하는데 높은 정확도를 보인다. 추가적으로, 기법은 높은 recall을 보이면서 약 275,000 정도의 중복을 발견하고 제거했다.

![2](https://user-images.githubusercontent.com/37301677/101244542-85fd2700-374a-11eb-88e3-472f69892389.PNG)
![3](https://user-images.githubusercontent.com/37301677/101244546-885f8100-374a-11eb-9f18-6167fa5199ae.PNG)

## 4.2  Faces

우리는 사람 이름의 랜점 웹 이미지 쿼리로부터 사람의 얼굴의 포함된 이미지들을 수집했다. 사람들의 이름은 dbpedia로부터 얻었고, 현대에 태어난 이름을 기주으로 했다. 이 데이터셋은 10K 사람들로부터 3M개의 이미지를 가진다. 우리는 이러한 이미지들에 OpenCV 얼굴 디텍터를 사용해서, 대략 350,000개의 얼굴 박스를 제공하면서 충분히 고해상도의 이미지를 검출했다. 우리는 이러한 얼굴 박스들을 학습에 사용했다. 이미지들에 데이터 어그멘테이션은 사용하지 않았다.

## 4.3 ImageNet-1K

우리는 Imagenet-1k를 비지도 학습을 위한 자연 이미지의 소스로 사용했다. 우리는 32x32 min-resized centor crops으로 학습했다. 이미지들에 데이터 어그멘테이션은 사용하지 않았다.

# 5. Empirical

## 5.1 Classifying CIFAR-10 using GANs as feature extractor

비지도 표현 학습 알고리즘의 퀄리티를 평가하는 하나의 일반적인 방법은 이것들을 지도 학습 데이터셋의 특징 추출기로써 적용하고 이러한 특징들에 맞는 선형 모델의 성능을 평가하는 것이다.

CIFAR-10 데이터셋에서, 매우 강력한 베이스라인 성능은 K-means와 같은 피쳐 학습 알고리즘을 사용하는 잘 조정된 단일 레이어의 특징 추출 파이프라인으로 부터 증명되었다. 매우 큰 피쳐맵을 사용할 때 이 방법은 80.6% 정확도를 보였다. 베이스 알고리즘의 비지도 다층 레이어 확장은 82% 정확도를 보였다. (Coates & Ng, 2011)  지도 학습의 task를 위한 DCGAN에 의해 학습된 표현의 퀄리티를 평가하기 위해서, 우리는 ImaegNet-1k를 사용하고 판별자의 모든 레이어로 부터 컨볼루션 피쳐들을 사용하여 학습하고, 각각의 레이어들에는 4x4 spatial grid를 생성하기 위해서 maxpooling을 한다. 이러한 피쳐들은 28672차원의 벡터로 flattend되고 concatenated 되고 regularized linear L2-SVM 분류기는 이들의 위에서 학습된다. 이것은 82.8% 정확도롤 보였고, 모든 K-mean 기반의 방법의 성능을 넘었다. 주목할만하게, 판별자는 K-means 기반의 방법과 비교하여 더 적은 피쳐 맵을 갖고 있지만, 4x4 spatial location의 많은 레이어들 때문에 총 피쳐 벡터 사이즈는 더 큰 결과를 얻었다. DCGAN의 성능은 특별하게 선택되고, 공격적으로 augement 되고, 소스 데이터셋으로 부터의 샘플을 구분하는 비지도 학습 방식의 Exemplar CNN 보다 더 작다. 판별자의 표현을 finetuning 해서 성능이 증가될 수 있지만, 우리는 이것은 차후의 일로 남겼다. 추가적으로, 우리의 DCGAN은 CIFAR-10에서 학습되지 않았기 때문에 이에 대한 실험은 학습된 피쳐의 도메인에 따른 강인함을 증명할 수 있을 것 이다.

![4](https://user-images.githubusercontent.com/37301677/101244550-8b5a7180-374a-11eb-898d-343bbabc73de.PNG)

## 5.2 Classifying SVHN Digits using GANs as a Feature Extractor

StreetView House Numbers (SVHN) 데이터셋에서, 우리는 레이블이 지정된 데이터가 부족할 때 지도 학습의 목적으로 DCGAN의 판별자의 특징을 사용했다. 이러한 데이터셋은 CIFAR-10 데이터셋에서의 실험 준비와 비슷하고, 추가적인 셋없이 10,000개의 샘플로부터 validation set을 분리했고 모든 하이퍼라미터와 모델을 선택하는데 사용했다. 1000개의 uniform 한 클래스 분포의 트레이닝 예시들은 랜덤하게 선택됬고 CIFAR-10에서 사용한 파이프라인과 같은 feature extraction에 regularized linear L2-SVM 분류기를 사용하여 학습하는데 사용되었다. 이것은 22.48% 테스트 오류를 보였고, 레이블이 없는 데이터를 이점으로 사용하도록 설계된 CNN의 변형들보다 성능이 더 높다. 추가적으로, 우리는 DCGAN에 사용된 CNN 아키텍쳐가 같은 구조에 같은 데이터에 64번의 하이퍼 파라미터 랜덤 탐색 시토를 통해 모델을 최적화하면서 순수하게 지도학습 CNN에 학습되는데 사용된다면 모델의 성능에 중요한 기여 요소가 아니라는 것을 검증했다. 이것은 28.87%로 더 높은 테스트 에러를 달성했다.

우리는 다양한 방법으로 학습된 생성자와 판별자를 조사했다. 우리는 트레이닝 셋에 대해 nearest neighbor search와 같은 종류는 사용하지 않았다. 픽셀 또는 특징 공간 안에서 nearest neighbors 는 작은 이미지 변형에 의한 사소한 속임수이다. 우리는 또한 로그우도 metric은 좀 안 좋은 metric이기 때문에 모델을 정량적으로 평가하는데 사용하지 않는다.

![5](https://user-images.githubusercontent.com/37301677/101244551-8bf30800-374a-11eb-8b0a-96e346228d94.PNG)

<h1>6. Inverstigating and Visualizing the Internals of the Networks</h1>

## 6.1 Walking in the Latent Space

우리가 진행한 첫번째 실험은 잠재공간의 랜드스케이프를 이해하는 것이다. 학습된 매니폴드 위를 걷는 것은 보통 우리에서 암기의 표시에 대해서나 공간이 계층적으로 무너진 방법을 말해준다. 잠재공간을 걷는 것이 이미시 생성에 의미있는 변화의 결과를 준다면 (오브젝트가 추가되거나 제거되는 것), 우리는 모델이 연관도어 있고 흥미로운 표현을 학습했다고 추측할 수 있다. 이 결과는 그림 4에서 보여준다.

## 6.2 Visualizing the Discriminator Features

이전의 연구들은 큰 이미지 데이터셋에 대한 CNN의 지도 학습은 매우 강력한 학습된 피쳐의 결과들을 증명해왔다. 추가적으로, scene classification에 사용된 지도된 CNN은 object detector들을 학습한다. 우리는 큰 이미지 데이터셋에 대한 비지도 DCGAN 또한 흥미로운 특징의 계층을 학습한다는 것을 증명했다. (Springenberg et al 2014)가 제안한 guided backpropagation을 사용하여, 우리는 그림 5에서 판별자에 의해 학습된 특징이 침대와 창문 같은 전형적인 침실 부분에서 활성화 되는 것을 볼 수 있다. 비교를 위해, 같은 그림에서, 우리는 랜덤하게 초기화된 피쳐들의 베이스라인을 보여주는데 이것은 의미적으로 연관되거나 흥미로운 부분에 활성화되지 않는 것을 보였다.

## 6.3 Manipulating the Generator Representation

### 6.3.1 Forgetting to Draw Certain Objects

판별자에 의해 학습된 표현 외에도, 생성자가 학습하는 표현은 무엇인지 의문이 있다. 샘플의 퀄리티는 생성자가 주요한 장면 요소를 위한 침대, 창문, 등불, 문, 그리고 여러가지 잡다한 가구 같은 특정한 오브젝트 표현을 학습한다는 것을 제안한다. 이러한 표현들이 가지는 형태를 살펴보기 위해서, 우리는 생성자로부터 생성된 창문들을 완전하게 제거하는 시도를 하는 실험을 수행했다.

150개의 샘플에 대해 52개의 창문 바운딩 박스들을 매뉴얼하게 그렸다. 두번째로 큰 컨볼루션 레이터 특징들에서, 로지스틱 회귀는 피쳐의 활성이 바운딩 박스 안에 그려진 것은 positive이고 같은 이미지로 부터의 랜덤한 샘플은 negative로 활성화하는 기준에 의해 창문에 대해 되는지 아닌지 예측하기 위해 맞춰진다. 이러한 간단한 모델을 사용해서, zero보다 큰 값을 가진 가중치의 모든 피쳐 맵들은 모든 spatial location으로 부터 제거되었다. 그리고, 새로운 랜덤 샘플들은 피쳐 맵의 제거 하고, 그리고 제거 안하고 생성 되었다.

창문 제거 있이 그리고 없이 생성된 이미지는, 흥미롭게도, 네트워크는 침실 안에서 창문을 그리는 것을 대부분 잊었고, 이것을 다른 오브젝트로 대체했다.

![6](https://user-images.githubusercontent.com/37301677/101244552-8c8b9e80-374a-11eb-89b2-9a862b4b0c42.PNG)

![7](https://user-images.githubusercontent.com/37301677/101244553-8eedf880-374a-11eb-80e1-15ef28b7952d.PNG)
![8](https://user-images.githubusercontent.com/37301677/101244554-901f2580-374a-11eb-8117-0fdda58d713b.PNG)

### 6.3.2 Vector Arithmetic on Face Samples

단여의 표현을 학습하는 것을 평가하는 부분에서는 간단한 산술 연산이 표현 공간 안에서 풍부한 선형 구조를 드러낸다는 것을 증명했다. 하나의 예시는 vector("King") - vector("Man") + vector("Woman") 의 결과는 가장 근접한 이웃의 벡터인 Queen의 결과를 보이는 것을 증명했다. 우리는 우리의 생성자의 Z 표현에도 비슷한 구조를 가지는지 조사했다. 우리는 시각적 컨셉의 예시 샘플의 집합인 Z vector에 대한 비슷한 산술 연산을 수행했다. 컨셉 당 오직 하나의 샘플을 사용하는 것은 안정적이지 않지만, 3개의 예시에 대한 Z 벡터의 평균은 산술 연산을 의미적으로 따르면서 지속적이고 안정적인 생성을 보여주었다. Object manipulation에 대해서, 우리는 얼굴 포즈 또한 Z 공간 안에서 선형적으로 모델 될 수 있다는 것을 증명했다.

이러한 증명들은 우리의 모델에 의해 학습된 Z 표현을 사용해서 흥미로운 어플리케이션들을 개발할 수 있다는 것을 제안한다. 이전부터 conditional generative model들은 스케일, 회전, 포지션 같은 오브젝트 특징들을 학습할 수 있다는 것을 증명해왔다. 우리가 아는 한에서는 본 연구는 순수한 비지도 학습 모델에서 발생하는 첫번째 증명이다. 위에서 언급된 벡터 산술 너머 탐색하는 것과 개발하는 것은 복잡한 이미지 분포에 대한 conditional generative modeling을 위해 필요한 데이터 양을 매우 줄일 수 있다.

# 7. Conclusion and Future Work

우리는 generative adversarial network를 학습하기 위한 더 안정적인 아키텍쳐를 제안하고 우리는 적대적 네트워크가 지도 학습과 generative modeling을 위한 이미지 표현도 잘 학습할 수 있다는 증명을 했다. 아직 모델의 불안정성이 남아있다. 모델을 오래 학습하는 것은 필터의 부분집합들이 때때로 단일의 oscillating mode로 붕괴되는 것을 볼 수 있다. 다음 연구는 이러한 불안정성의 형태를 다룰 필요가 있다. 우리는 이 프레임워크를 비디오, 오디오와 같은 다른 도메인에 확장하는 것도 매우 흥미로울 것이라 생각한다. 잠재공간 학습의 특성에 대한 더 깊은 조사 또한 흥미로울 것이다.

![9](https://user-images.githubusercontent.com/37301677/101244556-91505280-374a-11eb-88e0-ff0a841b6663.PNG)
![10](https://user-images.githubusercontent.com/37301677/101244558-91e8e900-374a-11eb-9721-2b43b6862704.PNG)

<br>

지금까지 DCGAN 논문 리뷰를 해보았습니다. 저도 논문을 읽으며 공부해나가는 단계라 부족한 점이 있을 수 있습니다. 같이 잘 공부해나가요! 질문이나 지적, 요청해주실 부분이 있다면 댓글이나 메일 부탁드립니다.

읽어주셔서 감사합니다. 😃

