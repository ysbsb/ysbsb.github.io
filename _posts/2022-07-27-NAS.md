---
layout: post
title: NAS 논문 리뷰 - Neural Architecture Search with Reinforcement Learning (ICLR 2017)
subtitle: NAS paper review - Neural Architecture Search with Reinforcement Learning (ICLR 2017)
category: [NAS]
tags: [NAS]
comments: true
use_math: true

---





> [![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fysbsb.github.io%2Fgan%2F2022%2F03%2F14%2FBEGAN.html&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)
>
> 안녕하세요. 모카의 머신러닝 입니다. 이번 포스팅은 "Neural Architecture Search with Reinforcement Learning, ICLR 2017"에 대해 리뷰합니다. 영어로 된 논문을 한글로 같이 해석하며 논문에서 의미하는 것이 무엇인지 같이 공부하며 이해하고 활용하실 수 있도록 준비해보았습니다. 

<br>



[Neural Architecture Search with Reinforcement Learning](https://openreview.net/forum?id=r1Ue8Hcxg)





논문에 대한 해석은 간결한 문장으로 표현하려고 합니다. 그림과 표는 모두 해당 논문을 참조하였습니다.

<br>



# Abstract

뉴럴 네트워크는 이미지, 스피치, 자연어 이해와 같은 많은 어려운 학습 문제에 잘 작동하는 강력하고 유연한 모델이다. 이들의 성공에도 불구하고, 뉴럴 네트워크는 아직도 설계하기 어렵다. 본 논문에서, 우리는 뉴럴 네트워크의 모델 설명을 생성하기 위해서 recurrent network를 사용하고 validation set에서 생성된 아키텍쳐들의 예측된 정확도를 최대화시키기 위해 RNN을 강화학습으로 학습한다. CIFAR10 데이터셋에서, 스크래치부터 시작하는 우리의 방법은 테스트셋 정확도의 관점에서 사람이 발명한 최고의 아키텍쳐와 견주는 참신한 네트워크 아키텍쳐 설계가 가능하다. 

# 1. Introduction

지난 몇 년 동안 스피치 인식, 이미지 인식, 기계 번역과 같은 많은 도전적인 적용들에서 딥뉴럴 네트워크는 많은 성공을 이루어왔다. 이러한 성공은 SIFT, HOG부터 AlexNet, VGGNet, GoogleNet, ResNet까지 feature 설계로부터 아키텍쳐 설계로의 패러타임 이동과 함께 해왔다. 더 쉬워졌음에도 불구하고, 아키텍쳐 설계는 아직도 많은 전문가의 지식이 필요하고 충분한 시간이 걸린다.

![5](https://user-images.githubusercontent.com/37301677/181137538-b97a4d23-d582-4911-9aa4-d39cb3f1fb7a.png)

본 논문은 좋은 아키텍쳐를 찾기 위한 gradient-based 방법인 Neural Architecture Search를 제안한다. 우리의 연구는 뉴럴 네트워크의 구조와 연결성이 길이가 변경가능한 문자열로 구체화 될 수 있다는 관찰에 기초해있다. 따라서 이와 같은 문자열을 생성하기 위해서 recurrent network를 사용하는 것이 가능하다. 네트워크를 학습하는 것은 validation set에서 정확도의 결과를 얻는 실제 데이터에서의 문자열에 의해 구체화된다. 이 정확도를 리워드 신호로써 사용하여, 우리는 컨트롤러를 업데이트하기 위해서 policy gradient를 계산할 수 있다. 결과적으로, 다음 interation에서, 컨트롤러는 높은 정확도를 얻는 아키텍쳐에 높은 확률을 제공한다. 다른 말로, 컨트롤러는 시간에 따라 search를 향상시키도록 배울 수 있다.

# 3. Method

## 3.1 Generate Model Descriptions with Controller Recurrent Neural Network

Neural Architecture Search에서, 우리는 뉴럴 네트워크의 아키텍쳐의 하이퍼파라미터를 생성하기 위해서 컨트롤러를 사용할 수 있다. 좀 더 유연하게, 컨트롤러는 recurrent neural network로 구현될 수 있다. 오직 convolutional layer들만 사용하여 feedforwad neural network를 예측한다고 가정해보자, 우리는 하이퍼파라미터를 시퀀스로서 생성하기 위해 컨트롤러를 사용할 수 있다.

![6](https://user-images.githubusercontent.com/37301677/181137582-d290d259-4e00-4c8d-8f83-4d10863e78c5.png)

우리의 실험에서, 아키텍쳐를 생성하는 과정은 레이어의 수가 특정한 값을 초과할 때 멈춘다. 이 값은 우리가 학습 과정에서 이것을 상승시킬 때 스케쥴을 따른다. RNN이 한번 아키텍쳐 생성을 끝내고 하면, 이 아키텍쳐로 뉴럴 네트워크를 구축하고 학습한다. 수렴하면, validation set에서 네트워크의 정확도가 기록된다. 컨트롤러 RNN의 파라미터는 제안된 아키텍쳐의 예측된 validation 정확도를 최대화 시키기 위해 최적화된다. 다음 섹션에서는, 우리는 파라미터를 업데이트 해서 컨트롤러 RNN이 시간에 따라 더 좋은 아키텍쳐를 생성하기 위한 policy gradient 방법을 설명할 것이다.

## 3.2 Training with REINFORCE

컨트롤러가 예측하는 토큰의 리스트들은 child network를 위한 아키텍쳐 설계의 action의 리스트들로 볼 수가 있다. 수렴하면, child network는 데이터셋에서 정확도 R을 얻을 수 있다. 우리는 이 정확도 R을 리워드 신호로서 사용할 수 있고 컨트롤러를 학습시키기 위해 강화학습을 사용할 수 있다. 더 구체적으로, 최적의 아키텍쳐를 찾기 위해서 우리는 컨트롤러가 예측된 리워드를 최대화하도록 할 수 있다.

![1](https://user-images.githubusercontent.com/37301677/181137621-09acb235-0cd5-487e-9178-f5c25cd511d8.png)

리워드 신호 R은 미분이 불가능하기 때문에, 우리는 theta를 반복적으로 업데이트하기 위해서 policy gradient 방법을 사용할 필요가 있다. 본 연구에서, 우리는 Williams (1992)로부터의 REINFORCE 방법을 사용한다.

![2](https://user-images.githubusercontent.com/37301677/181137624-3b2610cc-d8af-452c-ad36-33fec1fdcb46.png)

위의 quantity의 실험적 근사키는 다음과 같다.

![3](https://user-images.githubusercontent.com/37301677/181137626-c81332a7-551f-45c2-b10f-0a943ecf861f.png)

m은 하나의 배치에서 컨트롤러가 샘플하는 다른 아키텍쳐의 수이고, T는 우리의 컨트롤러가 뉴럴 네트워크 아키텍쳐를 설계하기 위해 예측하는 하이퍼파라미터의 수이다.

k번째의 뉴럴 네트워크 아키텍쳐가 학습 셋에 대해 학습이 된 후에 얻는 validation 정확도는 R_k 이다.

위의 업데이터는 우리의 그래디언트에 대해 편향되지 않은 예측이지만, 매우 높은 편차를 가지고 있다. 이 측정의 편차를 줄이기 위해서 우리는 베이스라인 함수를 사용한다.

![4](https://user-images.githubusercontent.com/37301677/181137629-88a27916-2f63-4d16-883b-548d37690c5b.png)

베이스라인 함수 b가 현재의 action에 의존하지 않는 한, 이것은 편향되지 않은 그래디언트 측정으로 남아있을 것이다. 본 연구에서는, 우리의 baseline b는 이전의 아키텍쳐의 정확도에 대한 exponential moving average이다.

### Accelerate Training with Parallelism and Asynchronous Updates

Neural Architecture Search에서, 컨트롤러 파라미터 theta를 위한 각각의 그래디언트 업데이트는 수렴을 위한 하나의 child network를 학습하는 것과 상응한다. child network를 학습시키는데에 몇 시간이 걸리는데, 우리는 컨트롤러의 학습 과정을 빠르게 하기 위해서 분산 학습과 비동기적 파라미터 업데이트를 사용한다. 우리는 S의 파라미터 서버를 가진 parameter-server 방법을 사용하고, K 컨트롤러 복제본을 위한 공유된 파라미터를 저장한다. 각각의 컨트롤러 복제본은 병렬적으로 학습되는 m개의 다른 child 아키텍쳐들을 샘플한다. 컨트롤러는 수렴에서 m개의 아키텍쳐의 미니배치의 결과에 따르는 그래디언트를 모으고 모든 컨트롤러 복제본에 걸쳐 가중치를 업데이트하기 위해서 파라미터 서버로 이것들을 보낸다. 우리의 구현에서, 각각의 child 네트워크의 수렴은 학습이 특정한 수의 epoch를 넘길떼 도달한다. 병렬적인 방법은 Figure 3와 같이 요약된다.

![7](https://user-images.githubusercontent.com/37301677/181137734-29da3f66-099a-4118-baaa-b9cf0225d2c2.png)

## 3.3 Increase Architecture Complexity with Skip Connetions and Other Layer Types

섹션 3.1에서, search space는 skip connetion이나 GoogleNet, Residual Net과 같은 현대의 아키텍쳐들에서 사용되는 branching layer들을 가지고 있지 않다. 이 섹션에서는 우리는 우리의 컨트롤러가 skip connetion이나 brancing layer를 제안하기 위고 따라서 search sapce아 넓도록 허락하는 방법을 소개한다.

컨트롤러가 이러한 connection들을 예측하는 것을 가능하게 하기 위하여, 우리는 어텐션 메커니즘에 기초한 set-selection 타입의 어텐션을 사용한다. Layer N에서, 우리는 연결이 필요한 이전 레이어를 지시하는 N-1 개의 컨텐츠 기반의 시그모이드를 가진 achor point를 더한다. 각각의 시그모이드 함수는 컨트롤러의 현재의 hiddenstate와 이전의 N-1 개의 anchor points의 이전의 hiddenstate의 함수이다:

![8](https://user-images.githubusercontent.com/37301677/181137763-e7f53312-eb75-4652-9962-eaafbcf4cc73.png)

h_j는 j가 0부터 N-1까지의 범위일 때, j번째 레이어의 anchor point에서 컨트롤러의 hiddenstate를 대표한다. 우리는 그 다음 무엇이 이전 레이어가 현재 레이어의 입력으로 사용되는지 결정하기 위해 시그모이드로부터 샘플한다. W_prev, W_curr, v는 학습가능한 파라미터이다. 이러한 connetion들이 확률 분포로써 정의되어씩 때문에, REINFORCE 방법은 상단한 변화없이 적용한다. Figure 4는 현재 레이어의 입력으로 원하는 레이어를 결정하기 위해서 어떻게 컨트롤러가 skip connetion들을 사용하는지 보여준다.

![9](https://user-images.githubusercontent.com/37301677/181137784-2a26c2ca-d22f-41c4-bb20-087a3d634851.png)

우리의 프레임워크에서, 하나의 레이어가 많은 입력 레이어를 가지고 있으면 모든 입력 레이어들은 depth dimension으로 concatenate 된다. Skip connection은 하나의 레어어가 또 다른 레이어와 호환이 불가능하거나 또는 하나의 레이어가 입력 혹은 출력을 가지고 있지 않을 때 “compilation failures”가 발생한다. 이러한 문제를 피하기 위해서, 우리는 3개의 간단한 테크닉을 사용한다. 첫번째로, 만약 레이어가 어떤 인풋 레이어에 대해서도 연결이 되어있지 않으면 이미지는 입력 레이어로써 사용된다 두번째로, 마지막 레이어에서 우리는 아웃풋을 마지막 hiddenstaet로부터 classifier로 보내기 전에 연결과 concatenate이 되지 않은 모든 레이어 아웃풋을 가진다. 마지막으로, 만약 입력 레이어가 다른 사이즈로 concatenate 된다면, 우리는 concatenate된 레이어가 같은 사이즈를 같도록 작은 레이어들을 0으로 pad 한다.

Section 3.1에서는, 우리는 learning rate를 예측하지 않고, 매우 제한적인 아키텍쳐가 오직 convolutional layer로 구성되어 있다고 가정한다. learning rate를 예측의 하나로 추가하는 것이 가능하다. 추가적으로, 아키텍쳐의 pooling, local contrast normalization, batchnorm을 예측하는 것도 가능하다. 더 많은 타입의 레이어들을 추가흐는 것을 가능하게 하기 위해서, 우리는 레이어 타입을 예측하기 위해서 controller RNN에서의 추가적인 스텝을 더하고, 그 다음 다른 하이퍼파라미터가 연관되었다.

## 3.4 Generate Recurrent Cell Architectures

본 섹션에서는, 우리는 recurrent cell을 생성하기 위해서 위의 방법을 변형할 것이다. 

# 4. Experiments and Results

우리는 딥러닝에서 가장 벤치마크된 두개의 데이터셋인 CIFAR10의 이미지 분류와 Pen Treebank의 언어 생성에 우리의 방법을 적용한다.  CIFAR10에서, 우리의 목표는 좋은 convolutional architecture를 찾는 것인 반면에 Pen Treebanck에서 우리의 목표는 좋은 recurrent cell을 찾는 것이다. 각각의 데이터셋에서, 우리는 리워드 시그널을 계산하기 위해서 별도의 validation 데이터셋을 가지고 있다. 테스트셋에서 리포트 된 성능은 별도의 validation 데이터셋에서 가장 좋은 결과를 얻는 네트워크에 단 한번 계산되었다.

## 4.1 Learning Convolutional Architectures for CIFAR10

### Dataset

이 실험에서 우리는 다른 이전의 결과와 같은 선상에 있는 데이터 전처리와 augmentation과 함께 CIFAR10을 사용했다. 우리는 처음으로 모든 이미지를 백색화함으로써 데이터 전처리를 한다. 추가적으로, 우리는 각각의 이미지를 업샘플하고 이 업샘플된 이미지에 대해서 32x32의 랜덤 크롭을 선택한다. 마지막으로, 우리는 이 32x32의 크롭된 이미지에 random horizontal flip을 사용한다.

### Search space

우리의 search space는 convolutional architecture, non-linearites의 rectified linear units, batch normalization, 그리고 레이어 사이의 skip connection으로 구성되었다. 매 컨볼루션 레이어마다, 컨트롤러 RNN은 filter height [1,3,5,7], filter width [1,3,5,7], 필터의 수 [24,36,48,64]를 선택한다. stride를 위해, 우리는 두가지 셋팅의 실험을 수행하고,  하나는 stride가 1로 고정돠었고, 다른 하나는 컨트롤러가 stride를 [1,2,3] 중에 예측하도록 허락한 것이다.

### Training details

컨트롤러 RNN은 각각의 레이어에 35개의 hidden unit을 가진 two-layer LSTM이다. 이것은 learning rate 0.0006에 ADAM 옵티마이저로 학습되었다. 컨트롤러의 가중치는 -0.08과 0.08 사이로 균등하게 초기화되었다. 분산 학습을 위해서, 어느 때나 동시에 800개의 GPU에 800개의 네트웤가 학습도리 수 있음을 의미하는 우리는 파라미터 서버 shards S이 수를 20으로, 컨트롤러 복제본의 수 K를 100으로, child 복제본의 수 m을 8로 설정했다.

컨트롤러 RNN이 아키텍쳐를 한번 샘플링하고 하면, child 모델은 구성되고 50 epoch을 위해 학습된다. 컨트롤러를 업데이트하기 위한 리워드는 마지막 5 epoch의 최대의 validation 정확도이다. validation 셋은 트레이닝 셋으로부터 5000개의 예시가 랜덤하게 샘플되었고 남은 45,00개의 예시들은 학습을 위해 사용된다. CIFAR10 child 모델을 학습하기 위한 셋팅은 Huang et al. (2016a)에서 사용된 것과 같다. 우리는 learning rate 0.1, weight decay 1e-4, momentum 0.9의 Momentum Optimizer를 사용했고 Nesterov Momemtum을 사용했다.

컨트롤러를 학습하는 동안, 우리는 학습 프로그래스로 child 네트워크에서의 레이어의 수를 증가시키는 스케줄을 사용했다. CIFAR10에서 우리는 컨트롤러가 6레이어로부터 시작해서 매 1600개의 샘플마다 child model에 대해 깊이를 2 증가시키도록 했다.

### Results

컨트롤러가 12,800개의 아키텍쳐를 학습한 후에, 우리는 아키텍쳐를 찾았고 가장 좋은 validation 정확도를 달성했다. 우리는 learning rate, weight decay, batchnorm epsilon, 그리고 어떤 epoch에서 learning rate을 decay 하는지에 따라 작은 grid search를 진행했다. 이 grid search로부터의 가장 좋은 모델은 수렴할 때까지 작동하고 그리고 우리는 이러한 모델의 테스트 정확도를 계산하고 Table 1에 결과를 요약했다. table에서 볼 수 있듯이, Neural Architecture Search는 여러개의 유망한 아키텍쳐를 설계할 뿐만 아니라 데이터셋에서 몇개의 최고의 모델을 수행할 수 있다.

![10](https://user-images.githubusercontent.com/37301677/181137821-1148c2af-dfec-47ed-84c0-92352b55eb25.png)

먼저, 우리가 컨트롤러에게 stride 또는 pooling을 예측하지 않도록 한다면, 이것은 테스트 셋에서 5.50%의 에러율을 달성하는 15개의 레이어의 아키텍쳐를 설계할 수 있다. 이러한 아키텍쳐는 정확도와 깊이 사이의 좋은 밸런스를 가진다. 사실상, 이것은 이 테이블의 가장 많이 수행하는 네트워크에 따라 가장 얕고 아마 가장 비용이 적은 아키텍쳐일 것이다. 이 아키텍쳐는 Appendix A와 Figure 7에서 보여진다. 이 아키텍쳐의 주목할만한 특징은 많은 직사각형 필터를 가지고 탑 레이어에서 큰 필터를 선호한다는 것이다. residual network와 같이, 아키텍쳐는 많은 one step skip connection이 있다. 예를 들어, 만약 우리가 skip connection과 함께 모든 레이어들을 연결하면, 이것의 성능은 더 안좋아질 것이다: 5.56%. 만약 우리가 모든 skip connection을 제거한다면, 성능느 7.97로 저하된다.

실험의 두번쩨 셋에서, 우리는 컨트롤러가 다른 하이퍼파라미터와 더불어 stride를 예측하도록 했다. 앞서 언급했듯이, 이것은 search space가 커지는 일이기 때문에 더 도전적인 문제이다. 이 경우에, 이것은 실험의 섯번째 셋보다 더 크게 적지 않으며 테스트 셋에서 6.01%의 에러율을 달성하며 20개의 레이어의 아키텍쳐를 찾는다.

마지막으로, 만약 우리가 컨트롤러가 아키텍쳐의 레어어 13과 레어어 24에 2개의 pooling 레이어들을 포함시키도록 한다면, 컨트롤러는 3.74%를 달성하는 사람이 개발한 최고의 아키텍쳐에 가깝게 4.47%를 달성하여 39개의 레이어의 네트워크를 설계할 수 있다. search space complexity를 제한하기 위해서, 우리는 각각의 레이어 예측이 3 레이어의 fully connected blobk인 13개의 레이어를 예측하는 모델을 가진다. 추가적으로, 우리는 우리의 모델의 필터의 수를 [24,36,48,64]에서 [6,12,24,36]으로 예측할 수 있도록 변경했다. 우리의 결과는 우리의 아키텍쳐의 각 레이어에 40개의 더 많은 필터를 추가함으로써 3.65%로 향상되었다. 추가적으로 40개의 필터가 추가된 이 모델은 3.74%를 달성하는 DenseNet보다 더 좋은 성능을 보이며 1.05배 빠르다. 우리는 그렇게 하지 않아 정확한 비교가 불가한, 전체의 파라미터 수를 줄이기 위해 1x1 컨볼루션을 사용한  DenseNEt 모델은 3.46%의 에러율을 보였다. 

# 5. Conclusion

본 논문은 뉴럴 네트워크 아키텍쳐를 구성하기 위해서 recurrent neural network를 사용하는 아이디어인 Neural Architecture Search를 소개한다. 컨트롤러로써 recurrent network를 사용함으로써, 우리의 방법은 유연하고 따라서 가변 길이의 아키텍쳐 space를 찾을 수 있다. 우리의 방법은 매우 도전적인 벤치마크에 대해 강력한 실험적인 성능을 가졌고 자동으로 좋은 뉴럴 네트워크 아키텍쳐를 찾을 수 있는 새로운 연구 방향을 제시한다. CIFAR10과 PTB에서의 컨트롤러에 의해 찾아진 모델을 작동하는 코드는 [https://github.com/tensorflow/models](https://github.com/tensorflow/models) 에 릴리즈 되었다. 추가적으로, 우리는 RNN cell을 TensorFlow에 NASCell이라는 이름 하에 사용하도록 추가하여, 다른 사람들이 쉽게 사용할 수 있다.



<br>



지금까지 NAS 논문 리뷰를 해보았습니다. 저도 논문을 읽으며 공부해나가는 단계라 부족한 점이 있을 수 있습니다. 같이 잘 공부해나가요! 질문이나 지적, 요청해주실 부분이 있다면 댓글이나 메일 부탁드립니다.

읽어주셔서 감사합니다. 😃

